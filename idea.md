# Hackathon Ideas â€” Health & Wellness + Financial Health (Using Opik)

Expanded ideas for **Health, Fitness & Wellness** and **Financial Health** tracks, with concrete ways to use Opik for evaluation, observability, and continuous improvement.

---

# TripWise MVP Structure â€” Travel Budget Agent

**What we are building:** **TripWise** â€” an AI-powered **travel budget agent** that helps users understand flight and hotel costs, get savings suggestions in plain language, and make informed travel spending decisions. Uses **Comet Opik + OpikAssist** for observability, evaluation, and experimentation.

Hackathon MVP. Optimize for: **Speed Â· Clarity Â· Agent behavior Â· Visible Opik usage.**  
NOT for production completeness.

---

## ğŸ¯ GOAL

Build an agent that:
- Takes **trip input** (route, dates, budget) or pasted booking/search text
- Suggests **flight and hotel options** with cost tradeoffs in plain language
- Explains **"You could save $X byâ€¦"** (e.g. flexible dates, nearby airports) without overwhelm
- Tracks user feedback and â€œacted on suggestionâ€
- Uses **Opik** for traces, metrics, evals, and A/B experiments

---

## ğŸ§± TECH STACK (STRICT)

- **Next.js** (App Router)
- **Node runtime ONLY** (no Edge)
- **PostgreSQL** (Supabase / Neon compatible) â€” optional for MVP; can start with in-memory or SQLite
- **Comet Opik** (tracing, metrics, evals, OpikAssist)
- **LLM** (OpenAI / Gemini / Grok) â€” provider must be swappable
- **Vercel-friendly** deployment

---

## ğŸš¨ DOMAIN SAFETY (IMPORTANT)

This app handles **travel financial decisions only** â€” no booking, no payments.

**HARD RULES:**
- NO investment advice
- NO product or lender recommendations
- NO medical or health advice
- NO EMR or bank integrations (user paste/upload only)

Use neutral language:
- â€œfollow-upâ€
- â€œsuggestionâ€
- â€œauditâ€
- â€œkeep / reduce / cancelâ€

---

## ğŸ§  CORE AGENT BEHAVIOR

1. User **enters trip details** (route, dates, budget) or **pastes** booking/search text
2. Agent **fetches or infers** flight and hotel options (costs, tradeoffs)
3. Agent **suggests** in plain language:
   - Cost breakdown (flights, hotels, ballpark total)
   - **"You could save $X byâ€¦"** (e.g. flexible dates, nearby airports, different stay)
   - No overwhelm â€” clear, scannable summary
4. User sees options + savings summary; can mark:
   - **Acted on suggestion** (e.g. booked with flexible dates)
   - **Ignored** / **Delayed**
5. User gives **feedback** (e.g. thumbs up/down)
6. Agent **adapts** explanation style via A/B (e.g. save $X on this trip vs spending $X on flights)

This MUST behave like an **agent**, not a static price list.

---

## ğŸ—‚ï¸ REQUIRED PROJECT STRUCTURE

```
/app
  page.tsx              â†’ TripWise landing (hero + CTA "Plan trip")
  plan/page.tsx         â†’ Trip input + agent suggestions (flight/hotel)
/api
  generate-suggestions/route.ts â†’ Agent: trip â†’ options + savings in plain language
  (Optional) update-feedback/route.ts, progress/route.ts
/lib
  llm/
    provider.ts   â†’ unified LLM interface
    openai.ts
    gemini.ts
    grok.ts
  opik/
    client.ts     â†’ Opik initialization
    logTrace.ts
    logMetric.ts
    logEval.ts
  db.ts
  experiments.ts â†’ A/B experiment logic (variant assignment)
  prompts.ts     â†’ agent prompts
/sql (or prisma/)
  schema.sql
```

---

## ğŸ—ƒï¸ DATABASE (MINIMAL)

Tables:

- **users** â€” `id`, `email?`, `created_at`
- **trips** â€” `id`, `user_id`, `from`, `to`, `dates`, `budget`, `raw_text?`
- **suggestions** â€” `id`, `trip_id`, `summary`, `alternatives_json`, `savings_plain_text`, `variant`
- **feedback** â€” `suggestion_id`, `score`, `acted`, `created_at`

Keep schema small and readable.

---

## ğŸ¤– AGENT PROMPT REQUIREMENTS

The agent must:
- Use **trip input** (route, dates, budget or raw text) to produce cost breakdown and savings suggestions
- Avoid product names, investment advice, or medical language
- Output **plain-language** savings tips (e.g. "You could save $X byâ€¦") without overwhelming the user

Prompt must include:
- Trip/route/dates/budget (or pasted search text)
- User feedback history (if any)
- **Experiment variant (A/B)**

---

## ğŸ”€ A/B EXPERIMENT (REQUIRED)

Implement a simple experiment:

- **Variant A** â†’ Friendly tone, e.g. â€œYou could save $X on this trip byâ€¦â€
- **Variant B** â†’ Direct tone, e.g. â€œYouâ€™re spending $X on flights. Considerâ€¦â€

Assignment example:

```ts
variant = userId % 2 === 0 ? "A" : "B"
```

This variant **MUST** be logged to Opik (trace metadata, experiment tracking).

---

## ğŸ” COMET OPIK INTEGRATION (CRITICAL)

### 1. Traces

Log every agent decision:
- Trip input parsing (route, dates, budget)
- Flight/hotel option retrieval or inference
- Cost breakdown and savings suggestion generation
- Plain-language summary output

Trace payload must include:
- `user_id` (or session_id)
- `session_id`
- `trip_id` (when created)
- `prompt` (or trip summary)
- LLM response
- `savings_plain_text` (or summary)
- `variant`
- `latency`
- `cost` (if available)

### 2. Metrics

Log:
- `trip_plan_completed` (0/1)
- `feedback_score` (e.g. thumbs)
- `acted_on_suggestion` (0/1)
- `completion_rate` (by variant)

### 3. Custom Evaluation

Implement a simple evaluation:
- Replay last N trip plans (or golden set)
- Compute completion / usefulness by:
  - trip type (route, budget range)
  - explanation style (variant A vs B)
- Log evaluation results to Opik

### 4. OpikAssist Readiness

Traces must be labeled so OpikAssist can answer:
- â€œWhy do savings suggestions get ignored more?â€
- â€œWhich explanation style gets more follow-through?â€

### 5. Anonymizers & Guardrails

- **Anonymizers** â€” Strip merchant names and amounts in production logs; only aggregate stats in Opik.
- **Guardrails** â€” Block â€œinvest the savingsâ€, booking/payment claims, or product names in agent output.

---

## â° CRON (OPTIONAL)

Implement `/api/cron/daily` (or weekly) that:
- Can send a â€œplan your next tripâ€ or savings reminder nudge (e.g. log to console or email if configured)
- Logs reminder actions to Opik
- Can be manually triggered for demo

---

## ğŸ”Œ LLM PROVIDER ABSTRACTION

Implement a clean adapter:

```ts
generateText(prompt: string): Promise<...>
```

Switch provider using:

```bash
LLM_PROVIDER=openai | gemini | grok
```

---

## ğŸš« HARD CONSTRAINTS

- NO authentication system (anonymous sessions OK)
- NO queues or background workers beyond cron
- NO mobile apps
- NO complex UI polish
- NO premature optimization

---

## âœ… MVP SUCCESS CRITERIA

By the end, the app must:
- Convert trip input into cost breakdown + savings suggestions in plain language
- Capture user feedback (thumbs, acted/didnâ€™t)
- Show **Opik traces & metrics**
- Support **A/B experiments** (variant logged)
- Be **demoable in under 3 minutes**

---

## ğŸ BUILD ORDER (DO NOT SKIP)

1. Create folder structure
2. Define database schema
3. Initialize Opik client
4. Build **trip input â†’ suggestions** flow (UI + API stub)
5. Add agent logic (LLM + variant in prompt)
6. Instrument **Opik traces & metrics**
7. (Optional) Add cron reminder
8. Verify **Opik dashboards** are populated

Do NOT skip Comet Opik instrumentation.

---

## ğŸƒ Health, Fitness & Wellness Track

### 1. Adaptive Workout Planner Agent
**Concept:** Multi-turn agent that designs workouts from goals, fitness level, equipment, and recovery state.

**Opik usage:**
- **Trajectory evaluation** â€” Score full workout-planning flows (clarity, safety, progression, personalization).
- **LLM-as-judge evals** â€” "Does this plan avoid overreach?" "Is rest/recovery adequate?" "Is guidance age- and condition-appropriate?"
- **Annotation queues** â€” Experts label plans as safe/unsafe, appropriate/inappropriate for later model tuning.
- **Experiment tracking** â€” Compare prompt variants (e.g. "conservative vs progressive") and track injury-risk proxy metrics.
- **Guardrails** â€” Block or anonymize medical/condition mentions in logs; add safety checks before suggesting intensity.

**Metrics to track:** Safety score, user adherence proxy, plan diversity, cost per plan.

---

### 2. Mood-Aware Meditation & Reflection Guide
**Concept:** Agent that suggests short meditation/breathing/reflection prompts based on time of day, self-reported mood, and optional journal snippets.

**Opik usage:**
- **Conversation logging** â€” Log full sessions (with user consent) to analyze which prompts improve self-reported mood.
- **Single-prompt & multi-turn evals** â€” "Is tone supportive and non-clinical?" "Does it avoid diagnosing or treating mental health?"
- **User feedback** â€” Thumbs up/down and optional ratings; log and use for online eval rules and retraining.
- **Agent Optimizer** â€” Tune system prompts so recommendations stay within "wellness, not therapy" and avoid harmful advice.
- **Dashboards** â€” Track usage by mood type, drop-off points, and feedback distribution.

**Metrics:** Helpfulness rating, safety (no clinical claims), session length, return rate.

---

### 3. Sleep Habit Analyzer & Coach
**Concept:** Agent that ingests sleep logs (manual or from wearables), explains patterns in plain language, and suggests small, sustainable habit changes.

**Opik usage:**
- **Multimodal evaluation** â€” If users upload charts/screenshots, evals on "Did the summary match the data?" and "Were suggestions grounded in the chart?"
- **Regression tests** â€” Pytest + Opik: compare behavior across model versions (e.g. "Never recommend <5h as OK", "Always caveat medical conditions").
- **Online evaluation rules** â€” Flag responses that mention medication, sleep disorders, or "diagnosis" for human review.
- **Experiment tracking** â€” A/B different explanation styles (short vs detailed) and track user comprehension and follow-through proxies.

**Metrics:** Factual accuracy vs input data, safety flags, user satisfaction, suggestion uptake.

---

### 4. Recovery & Readiness Coach
**Concept:** Agent that uses HRV, soreness, sleep, and workout history to suggest "train today light / rest / full" and simple recovery actions.

**Opik usage:**
- **Trajectory scoring** â€” Evaluate end-to-end flows: Did it ask enough context? Did it err on the side of caution?
- **Guardrails & anonymizers** â€” Strip or anonymize health identifiers in production logs.
- **Alerts** â€” Trigger review when the agent frequently suggests "train hard" after poor sleep or high stress.
- **Annotation queues** â€” Coaches label recommendations as appropriate/overreaching for eval and tuning.

**Metrics:** Conservative vs aggressive recommendation rate, expert agreement, user-reported outcomes.

---

### 5. Sustainable Routine Builder
**Concept:** Agent that helps users build weekly routines (exercise, meals, sleep, mindfulness) with gradual, sustainable changes.

**Opik usage:**
- **Agent evaluation** â€” Score plans on sustainability (small steps), balance across domains, and clarity.
- **Prompt Generator/Improver** â€” Iterate on instructions so outputs are actionable and avoid "all or nothing" language.
- **User feedback loops** â€” Log "stuck to it / didn't" and feed into online evals and reporting.
- **Cost tracking** â€” Monitor token use per user/session for scaling and optimization.

**Metrics:** Sustainability score (from evals), adherence signals, diversity of routines, cost per user.

---

### 6. Nutrition-within-Wellness Assistant
**Concept:** Explains food choices in simple terms, suggests swaps, and ties to energy/moodâ€”without giving medical or dietary prescriptions.

**Opik usage:**
- **Single-prompt evals** â€” "No specific calorie/medical targets?" "No absolutes like 'never eat X'?" "Culturally sensitive?"
- **Safety/false-positive tradeoffs** â€” Tune thresholds so we catch risky advice (e.g. extreme restriction) without over-flagging benign tips.
- **Production monitoring** â€” Dashboards for topics (e.g. "weight", "allergies") and flag spikes for review.

**Metrics:** Safety rate, false-positive rate on flags, user satisfaction, clarity score.

---

### 7. Hydration & Caffeine Timing Coach
**Concept:** Agent that logs daily water and caffeine intake (manual or from apps), explains how they affect energy/sleep, and suggests small timing tweaksâ€”never medical or supplement advice.

**Opik API usage:**
- **Log traces** â€” Record each â€œadd intakeâ€ / â€œget suggestionâ€ step for debugging and pattern analysis.
- **Log conversations + user feedback** â€” Full sessions plus thumbs/ratings; feed into **online evaluation rules** (e.g. flag when users report headaches or insomnia after suggestions).
- **Single-prompt evaluation** â€” LLM-as-judge: â€œDoes this avoid medical advice?â€ â€œNo recommendation of specific supplements or dosages?â€ â€œSuggestions grounded in stated intake?â€
- **Guardrails** â€” Block or redact specific product names and â€œdiagnosisâ€ language in logs; **anonymizers** for any health identifiers before storage.
- **Experiment tracking** â€” A/B â€œconservative vs moderateâ€ caffeine-cutoff advice; track user-reported sleep/energy as proxy metrics.

**Metrics:** Safety (no medical claims), suggestion acceptance rate, user-reported sleep/energy delta, cost per session.

---

### 8. Posture & Desk Ergonomics Coach
**Concept:** Agent that uses short text or optional uploads (e.g. desk selfie or room description) to suggest posture cues and micro-breaksâ€”no diagnosis of injury or pain.

**Opik API usage:**
- **Log media & attachments** â€” If users upload images, log them (with consent) for **multimodal evaluation**: â€œDid suggestions reference the image?â€ â€œNo diagnosis of injury?â€
- **Trajectory evaluation** â€” Score full flows: Did it ask about setup and discomfort level before advising? Did it err on â€œtake breaksâ€ rather than â€œpush throughâ€?
- **Annotation queues** â€” Ergo experts label outputs as safe/unsafe; build golden sets for **re-running experiments** when you change the model.
- **Alerts & dashboards** â€” Alert when phrases like â€œyou have X conditionâ€ or â€œyour spine isâ€¦â€ appear; dashboard for â€œsuggest breakâ€ vs â€œsuggest stretchâ€ ratio.
- **Prompt Playground** â€” Rapidly test edge inputs (â€œsharp pain in neckâ€, â€œpregnantâ€) and lock down safe, consistent disclaimers.

**Metrics:** Multimodal faithfulness, safety score, expert agreement, user completion rate.

---

### 9. Stress-Signal & Break Reminder Agent
**Concept:** Lightweight agent that infers stress from time of day, calendar density, or short self-report, then suggests 1â€“3 minute breathing or refocus promptsâ€”wellness only, never therapy.

**Opik API usage:**
- **Log conversations** â€” Every session; use for **Agent Optimizer** to tune prompts so tone stays supportive and non-clinical.
- **Multi-turn evaluation** â€” â€œDid it avoid diagnosing or treating anxiety/depression?â€ â€œWere suggestions time-bounded and actionable?â€
- **User feedback + online evaluation rules** â€” Thumbs and optional â€œfelt better / no changeâ€; rules to flag sessions where users say â€œmade me worseâ€ for review.
- **Cost tracking** â€” Per-session token use (agents run often); optimize for short, cheap prompts.
- **Production monitoring** â€” Dashboards by time-of-day, user return rate, and feedback distribution to improve timing and content.

**Metrics:** Safety (no clinical claims), â€œfelt betterâ€ rate, session length, cost per session.

---

### 10. Injury Prevention Checklist Agent
**Concept:** Pre-activity agent that runs a short checklist (warm-up done? any niggles? sleep/hydration?) and suggests â€œgo / modify / skipâ€ with optional exercise swapsâ€”never â€œpush through pain.â€

**Opik API usage:**
- **Trajectory evaluation** â€” End-to-end score: â€œDid it ask key safety questions?â€ â€œDid it never encourage ignoring pain?â€ â€œWere modifications concrete?â€
- **LLM-as-judge** â€” â€œRecommendation consistent with stated niggles?â€ â€œNo medical or injury diagnosis?â€ â€œAge/context appropriate?â€
- **Guardrails** â€” Block phrases like â€œpush throughâ€, â€œno pain no gainâ€ in production; **anonymizers** for body-part or condition mentions in logs.
- **Pytest integration** â€” Regression suite: â€œFor input â€˜sharp pain in kneeâ€™, output must suggest skip or see professionalâ€; run in CI on every model/prompt change.
- **Annotation queues** â€” Coaches label go/modify/skip decisions for eval and **Agent Optimizer** tuning.

**Metrics:** Consistency with stated risk factors, safety score, expert agreement, user adherence to â€œskipâ€ when suggested.

---

### 11. Mindful Eating & Meal-Mood Logger
**Concept:** Agent that helps users log meals and brief mood/energy (no calories required), surfaces patterns in plain language, and suggests one small swap or ritualâ€”no diets or weight targets.

**Opik API usage:**
- **Log conversations + user feedback** â€” Full flows and thumbs/ratings; drive **experiment tracking** (e.g. â€œsuggest one swapâ€ vs â€œsuggest two optionsâ€).
- **Single-prompt evaluation** â€” â€œNo calorie or weight targets?â€ â€œNo â€˜good/bad foodâ€™ framing?â€ â€œCulturally sensitive and non-stigmatizing?â€
- **Safety/false-positive tradeoffs** â€” Tune **online evaluation rules** so we flag extreme restriction language without over-flagging â€œeat more veggies.â€
- **Prompt Generator & Improver** â€” Iterate on wording so suggestions stay actionable and avoid â€œall or nothingâ€ language.
- **Dashboards** â€” Topic mix (e.g. â€œstress eatingâ€, â€œsleepâ€), suggestion acceptance, and feedback trends.

**Metrics:** Safety rate, false-positive rate on flags, suggestion acceptance, return rate.

---

### 12. Cycle-Aware Wellness Companion (e.g. period / hormonal)
**Concept:** Agent that uses cycle phase (if shared) plus sleep, energy, and appetite to suggest gentle routine tweaksâ€”focus on sustainable habits, never diagnosis or treatment.

**Opik API usage:**
- **Guardrails & anonymizers** â€” Strip or anonymize cycle and health identifiers in **all production logs**; block medical or fertility claims in outputs.
- **Trajectory evaluation** â€” â€œDid it stay within wellness (sleep, nutrition, gentle movement) and avoid medical advice?â€ â€œSuggestions proportional to userâ€™s stated energy?â€
- **LLM-as-judge** â€” â€œNo diagnosis?â€ â€œNo specific supplement or medication suggestion?â€ â€œRespectful and non-stigmatizing?â€
- **Alerts** â€” If outputs mention â€œsee a doctorâ€ or medical terms beyond a simple disclaimer, trigger human review.
- **Annotation queues** â€” Experts label outputs for appropriateness; use for golden set and **re-running experiments** when guidelines or model change.

**Metrics:** Safety (no medical overreach), user comfort score, suggestion relevance, guardrail trigger rate.

---

## ğŸ’° Financial Health Track

### 1. Plain-Language Financial Explainer Agent
**Concept:** Multi-turn agent that answers "Why did my bill go up?" "What's the difference between APR and APY?" using user's own statements or generic examples.

**Opik usage:**
- **LLM-as-judge evals** â€” "Is this accurate?" "Is it in plain language?" "Does it avoid speculation or investment advice?"
- **Trajectory evaluation** â€” Score full explanations for correctness, completeness, and appropriateness for stated literacy level.
- **Annotation queues** â€” Finance educators label answers for accuracy and suitability; use for golden sets and regression tests.
- **Re-running experiments** â€” When docs/regulations change, re-run evals on archived prompts to catch regressions.

**Metrics:** Accuracy score, reading-level score, "no speculation" compliance, user ratings.

---

### 2. Goal-Based Savings Assistant
**Concept:** Agent that helps set savings goals, suggests small weekly/monthly amounts, and tracks progress with encouragementâ€”no investment or product recommendations.

**Opik usage:**
- **Agent Optimizer** â€” Tune prompts so suggestions stay realistic (e.g. "save $X/week") and never cross into "put money in crypto/stock Y".
- **User feedback** â€” Log "helpful / not helpful" and "achieved goal / adjusted / abandoned"; feed into online eval rules.
- **Guardrails** â€” Block or redact product names, tickers, and specific investment advice in logs and outputs.
- **Dashboards** â€” Goal types, completion rates, and feedback trends per segment (e.g. by goal size).

**Metrics:** Advice appropriateness, guardrail triggers, goal completion rate, satisfaction.

---

### 3. Spending Tracker + Categorization Coach
**Concept:** Agent that categorizes transactions, explains spending patterns in simple language, and suggests achievable cutbacksâ€”no judgment, no product pitches.

**Opik usage:**
- **Multimodal evaluation** â€” If users upload statements (images/PDFs), evals on "Correct category?" and "Summary faithful to data?"
- **Regression test suite** â€” Pytest + Opik: "Never recommend borrowing for discretionary spend", "Always normalize for income level when comparing".
- **Online evaluation rules** â€” Flag any mention of loans, credit products, or "invest now" for review.
- **Experiment tracking** â€” Compare categorization prompts and explanation styles; track accuracy and user trust signals.

**Metrics:** Categorization accuracy, safety flags, user edits to categories, feedback scores.

---

### 4. Budget-Builder Chat Agent
**Concept:** Walks users through building a first budget (income, fixed vs flexible spending, savings) and answers follow-up questions in plain language.

**Opik usage:**
- **Multi-turn agent evals** â€” Score full dialogues: Did it cover essentials? Did it avoid risky advice? Was it encouraging?
- **Prompt Playground** â€” Rapidly test edge cases ("I have debt", "irregular income") and lock down safe, consistent answers.
- **Alerts** â€” If the agent often mentions "invest" or "borrow" in budget context, alert for review.
- **Cost tracking** â€” Track cost per budget "session" for efficiency and scaling.

**Metrics:** Completeness of budget coverage, safety score, drop-off points, user satisfaction.

---

### 5. "Why Does My Paycheck Look Like This?" Explainer
**Concept:** Agent that explains deductions (tax, benefits, retirement) in simple terms from a pay stub or user-described numbers.

**Opik usage:**
- **Single-prompt evaluation** â€” Correctness vs stub logic, plain language, no tax/legal advice disclaimer where needed.
- **Guardrails** â€” Anonymize dollar amounts and employer names in production logs.
- **Re-running experiments** â€” When tax rules change, re-evaluate old prompts against new policy docs.
- **Annotation queues** â€” HR/payroll experts validate explanations for a golden set.

**Metrics:** Explanation accuracy, safety/compliance rate, clarity score.

---

### 6. Financial Literacy Quiz & Coach
**Concept:** Short, conversational quiz on concepts (interest, compounding, budgeting); agent explains right/wrong answers and suggests one next concept.

**Opik usage:**
- **Trajectory evaluation** â€” Score flows: difficulty progression, explanation quality, no product pitches.
- **LLM-as-judge** â€” "Explanation correct?" "Appropriate for age/context?" "No speculative or risky framing?"
- **Experiment tracking** â€” Compare quiz order and explanation styles; optimize for comprehension and completion.
- **Production monitoring** â€” Track completion by topic, drop-off, and feedback to improve content.

**Metrics:** Completion rate, accuracy of explanations, user confidence gain, safety compliance.

---

### 7. Bill & Fee Negotiation Explainer
**Concept:** Agent that explains how to ask for lower rates (internet, phone, insurance), what to say and whenâ€”no scripted â€œcancel everything,â€ no product pitches.

**Opik API usage:**
- **LLM-as-judge** â€” â€œAccurate and practical?â€ â€œPlain language?â€ â€œNo speculative â€˜youâ€™ll definitely get 50% offâ€™?â€ â€œAvoids pressuring user to cancel?â€
- **Trajectory evaluation** â€” Score full explanations: completeness, appropriateness for stated provider and context, no risky or aggressive tactics.
- **Guardrails** â€” Block or redact provider names and dollar amounts in **production logs**; block â€œthreaten to leaveâ€ or â€œfake cancelâ€ as primary advice.
- **Re-running experiments** â€” When carrier or regulation docs change, re-run evals on archived prompts to catch outdated advice.
- **Annotation queues** â€” Consumer advocates label answers for accuracy and appropriateness; build golden set for regression tests.

**Metrics:** Accuracy score, â€œno pressureâ€ compliance, user ratings, safety flags.

---

### 8. Debt Paydown Strategy Explainer (Avalanche vs Snowball)
**Concept:** Agent that explains avalanche vs snowball in plain language, helps users compare â€œpay high-interest firstâ€ vs â€œpay smallest balance firstâ€â€”no product or loan recommendations.

**Opik API usage:**
- **Log conversations** â€” Full flows; use for **Agent Optimizer** so explanations stay clear and never cross into â€œtake this loanâ€ or â€œconsolidate here.â€
- **Single-prompt evaluation** â€” â€œMath consistent with userâ€™s stated balances/APRs?â€ â€œPlain language?â€ â€œNo product or lender names?â€ â€œDisclaimer that this is education, not advice?â€
- **Pytest integration** â€” Regression tests: â€œFor given balances and APRs, avalanche recommendation must match high-interest-first logicâ€; run in CI.
- **Online evaluation rules** â€” Flag any mention of specific lenders, consolidation products, or â€œinvest instead of pay debtâ€ for review.
- **Cost tracking** â€” Per-session cost; keep explanations concise to scale to many users.

**Metrics:** Math accuracy, â€œno productâ€ compliance, user comprehension (e.g. quiz or follow-up), guardrail triggers.

---

### 9. Subscription & Recurring Charge Audit Agent
**Concept:** Agent that ingests transaction lists or bank/card exports (manual paste or CSV), identifies subscriptions and recurring charges, and suggests â€œkeep / reduce / cancelâ€ in plain languageâ€”no product pitches.

**Opik API usage:**
- **Log media & attachments** â€” If users upload CSVs or screenshots, log (with consent) for **multimodal evaluation**: â€œCategories correct?â€ â€œSummary faithful to data?â€ â€œNo PII leaked in logs?â€
- **Anonymizers** â€” Strip merchant names and amounts in **production logs**; only aggregate stats (e.g. â€œ3 streaming, 2 softwareâ€) for dashboards.
- **Regression test suite (Pytest + Opik)** â€” â€œNever recommend borrowing to pay subscriptionsâ€; â€œAlways normalize for stated income when judging â€˜too muchâ€™.â€
- **Experiment tracking** â€” A/B explanation style (â€œsave $X/monthâ€ vs â€œyouâ€™re spending $X on Yâ€) and track user actions (cancel vs keep) as proxy for usefulness.
- **Alerts** â€” If outputs mention â€œinvest the savingsâ€ or specific products, alert for review.

**Metrics:** Categorization accuracy, safety flags, user â€œacted on suggestionâ€ rate, PII leak rate (zero target).

---

### 10. Emergency Fund Calculator & Coach
**Concept:** Agent that helps users set an emergency fund target (e.g. 3â€“6 months expenses), suggests small weekly deposits, and tracks progressâ€”no investment or product recommendations.

**Opik API usage:**
- **Agent Optimizer** â€” Tune prompts so suggestions stay in â€œsave $X in a safe accountâ€ and never cross into â€œput in crypto/stock Yâ€ or â€œuse this high-yield product.â€
- **Log user feedback** â€” â€œHelpful / not helpful,â€ â€œachieved milestone / adjusted / pausedâ€; feed into **online evaluation rules** and dashboards.
- **Guardrails** â€” Block product names, tickers, and â€œinvest your emergency fundâ€ in outputs and logs.
- **Trajectory evaluation** â€” â€œDid it ask about income stability and expenses before suggesting months of savings?â€ â€œEncouraging without being pushy?â€
- **Cost tracking** â€” Per-session cost; optimize for short, repeatable conversations (e.g. â€œupdate balance â†’ get next milestoneâ€).

**Metrics:** Advice appropriateness, goal completion rate, guardrail triggers, user satisfaction.

---

### 11. Credit Score Myth Buster
**Concept:** Agent that answers â€œWhat affects my score?â€ â€œWill one late payment ruin it?â€ in plain languageâ€”no â€œfix your score fastâ€ or paid-product pitches.

**Opik API usage:**
- **LLM-as-judge** â€” â€œFactually accurate?â€ â€œNo guarantee of specific score improvement?â€ â€œNo paid repair-service or product recommendations?â€ â€œAppropriate disclaimer that this is education?â€
- **Guardrails** â€” Block â€œboost your score in 30 days,â€ â€œwe can fix it,â€ or product names in outputs; **anonymizers** for any score numbers or bureau names in logs.
- **Re-running experiments** â€” When bureau rules or reporting changes, re-eval archived Q&A against new docs.
- **Prompt Playground** â€” Test edge questions (â€œIâ€™m in collections,â€ â€œIâ€™m disputingâ€) and lock down safe, consistent answers and disclaimers.
- **Annotation queues** â€” Credit educators label answers for accuracy; use for golden set and regression tests.

**Metrics:** Accuracy score, â€œno productâ€ compliance, user ratings, guardrail trigger rate.

---

### 12. Tax Withholding & W-4 Explainer
**Concept:** Agent that explains how withholding works, what W-4 choices mean in simple terms, and when to consider adjustingâ€”no specific tax or legal advice, with clear disclaimers.

**Opik API usage:**
- **Single-prompt evaluation** â€” â€œLogic consistent with stated income and allowances?â€ â€œPlain language?â€ â€œClear disclaimer that this is not tax/legal advice?â€ â€œNo promise of specific refund amount?â€
- **Guardrails** â€” Anonymize dollar amounts and employer names in **production logs**; block â€œyou will get $X backâ€ style guarantees.
- **Re-running experiments** â€” When IRS forms or guidelines change, re-run evals on archived prompts to catch regressions.
- **Annotation queues** â€” HR or tax educators validate explanations for a golden set.
- **Alerts** â€” If outputs give specific â€œchange line X to Yâ€ without disclaimer, trigger review.

**Metrics:** Explanation accuracy, disclaimer presence, safety/compliance rate, user satisfaction.

---

### 13. Family Money Conversations Coach (Parents & Kids)
**Concept:** Agent that helps parents explain earning, saving, and spending to kids in age-appropriate, honest languageâ€”no product pitches or â€œbuy this for your kids.â€

**Opik API usage:**
- **Trajectory evaluation** â€” â€œAge-appropriate language?â€ â€œNo product or brand recommendations?â€ â€œEncourages dialogue, not scripted lectures?â€ â€œRespectful of different family structures?â€
- **LLM-as-judge** â€” â€œExplanation correct?â€ â€œNo speculative or risky framing?â€ â€œSuitable for stated age range?â€
- **Guardrails** â€” Block specific product names, â€œinvest for your kid,â€ or high-pressure sales language in outputs and logs.
- **Experiment tracking** â€” Compare conversation frameworks (e.g. allowance vs goals) and track parent feedback and completion.
- **Production monitoring** â€” Dashboards by age bucket, topic (saving vs spending), and feedback to improve content.

**Metrics:** Age-appropriateness score, â€œno productâ€ compliance, parent satisfaction, completion rate.

---

## ğŸ”— Cross-Track Opik Patterns

| Need | Opik feature | Example |
|------|----------------|---------|
| "Is this safe / appropriate?" | LLM-as-judge evals | Health: no clinical claims; Finance: no speculation |
| End-to-end quality | Trajectory / agent eval | Workout plans, budget-building chats |
| Improve over time | Agent Optimizer + experiment tracking | Tune prompts for safety and helpfulness |
| Catch regressions | Pytest + re-run experiments | New model or policy â†’ re-eval old prompts |
| Production safety | Guardrails, anonymizers, alerts | Redact PII, block harmful topics, alert on spikes |
| Human-in-the-loop | Annotation queues | Expert labels for training and eval sets |
| Real user signal | User feedback + online eval rules | Thumbs, ratings, "achieved goal" as live metrics |

Use these in **hackathon-info.md** for structure and in **idea.md** for brainstorming and design. Prioritize a few Opik features deeply (e.g. trajectory evals + Agent Optimizer + one guardrail) so the demo clearly shows "evaluation loops that actually improve quality."

---

## ğŸ¯ Opik-First Quick Picks (Best Use of Opik)

Ideas that stack **multiple Opik API features** and show evaluation loops that improve quality:

| Idea | Opik stack to highlight | Why it shines for â€œBest Use of Opikâ€ |
|------|-------------------------|--------------------------------------|
| **Injury Prevention Checklist** (Health #10) | Trajectory eval + LLM-as-judge + Guardrails + Pytest + Annotation queues | End-to-end safety: evals, regression tests, human labels, and guardrails in one flow. |
| **Cycle-Aware Wellness Companion** (Health #12) | Guardrails + Anonymizers + Trajectory eval + Alerts + Annotation queues | Sensitive domain: anonymizers, guardrails, and human review baked in. |
| **Debt Paydown Explainer** (Finance #8) | Agent Optimizer + Single-prompt eval + Pytest + Online eval rules | Math correctness + â€œno productâ€ safety + regression suite and live rules. |
| **Subscription Audit Agent** (Finance #9) | Multimodal eval + Anonymizers + Pytest + Experiment tracking | Data-heavy: multimodal evals, PII protection, and A/B on explanation style. |
| **Emergency Fund Coach** (Finance #10) | Agent Optimizer + User feedback + Guardrails + Trajectory eval | Clear â€œtune prompts â†’ stay in scope â†’ re-evalâ€ loop with guardrails. |
| **Stress-Signal & Break Reminder** (Health #9) | Agent Optimizer + Multi-turn eval + User feedback + Cost tracking | High-volume, low-cost agent with feedback-driven tuning and cost visibility. |

**Demo tip:** Pick one idea and implement **trajectory eval + Agent Optimizer + one guardrail** end-to-end so judges see â€œevaluation loops that actually improve quality.â€
